{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "500193ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TUH TUSZ SEIZURE CORPUS - COMPREHENSIVE DATASET ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "[1/7] Scanning dataset structure...\n",
      "âœ“ Scan complete: 22092 files found\n",
      "  - Patients: 675\n",
      "  - EDF files: 7364\n",
      "  - CSV files: 7364\n",
      "  - CSV_BI files: 7364\n",
      "\n",
      "[2/7] Parsing channel-level annotations (CSV files)...\n",
      "âœ“ Parsed 200 CSV files\n",
      "  - Unique seizure types: 5\n",
      "  - Channels with seizure activity: 22\n",
      "\n",
      "[3/7] Parsing seizure events (CSV_BI files)...\n",
      "âœ“ Parsed 200 CSV_BI files\n",
      "  - Total seizure events detected: 288\n",
      "\n",
      "[4/7] Analyzing EDF file metadata...\n",
      "âœ“ Analyzed 50 EDF files\n",
      "\n",
      "[5/7] Computing comprehensive statistics...\n",
      "\n",
      "[6/7] Generating comprehensive visualizations...\n",
      "âœ“ Visualizations saved to 'tuh_tusz_comprehensive_analysis.png'\n",
      "\n",
      "[7/7] Generating detailed report...\n",
      "\n",
      "================================================================================\n",
      "DETAILED TUH TUSZ DATASET REPORT\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š DATASET OVERVIEW:\n",
      "  â€¢ Total Files: 22,092\n",
      "  â€¢ EDF Files (EEG recordings): 7,364\n",
      "  â€¢ CSV Files (channel annotations): 7,364\n",
      "  â€¢ CSV_BI Files (seizure events): 7,364\n",
      "  â€¢ Unique Patients: 675\n",
      "  â€¢ Total Sessions: 7,364\n",
      "  â€¢ Recordings in Train: 4,667\n",
      "  â€¢ Recordings in Eval: 865\n",
      "  â€¢ Recordings in Dev: 1,832\n",
      "\n",
      "ðŸ¥ PATIENT STATISTICS:\n",
      "  â€¢ Total unique patients: 675\n",
      "  â€¢ Total sessions: 7,364\n",
      "  â€¢ Recordings per patient:\n",
      "    - Mean: 10.91 Â± 20.98\n",
      "    - Median: 2\n",
      "    - Range: 1-252\n",
      "  â€¢ Sessions per patient:\n",
      "    - Mean: 10.91 Â± 20.98\n",
      "    - Median: 2\n",
      "  â€¢ Patient age:\n",
      "    - Mean: 67.5 years\n",
      "    - Range: 13-999 years\n",
      "\n",
      "ðŸ§  SEIZURE ANNOTATION STATISTICS:\n",
      "  â€¢ Total seizure events (from CSV_BI): 288\n",
      "  â€¢ Unique seizure types: 5\n",
      "  â€¢ Most common seizure types:\n",
      "    - fnsz: 456 (47.3%)\n",
      "    - gnsz: 332 (34.4%)\n",
      "    - cpsz: 132 (13.7%)\n",
      "    - tcsz: 22 (2.3%)\n",
      "    - absz: 22 (2.3%)\n",
      "  â€¢ Seizure duration statistics:\n",
      "    - Mean: 222.91 seconds\n",
      "    - Median: 79.56 seconds\n",
      "    - Std: 351.50 seconds\n",
      "    - Range: 0.10-2592.00 seconds\n",
      "    - Percentiles:\n",
      "      * 25th: 22.16s\n",
      "      * 75th: 300.00s\n",
      "      * 90th: 601.00s\n",
      "\n",
      "ðŸ“ CHANNEL ANNOTATION ANALYSIS:\n",
      "  â€¢ Total channels with annotations: 22\n",
      "  â€¢ Most active channels (seizure activity):\n",
      "    - T3-C3: 75 events\n",
      "    - T5-O1: 66 events\n",
      "    - C3-CZ: 66 events\n",
      "    - C3-P3: 62 events\n",
      "    - P3-O1: 61 events\n",
      "    - T3-T5: 59 events\n",
      "    - F3-C3: 55 events\n",
      "    - F7-T3: 54 events\n",
      "    - CZ-C4: 43 events\n",
      "    - F4-C4: 41 events\n",
      "\n",
      "âš–ï¸ CLASS BALANCE ANALYSIS:\n",
      "  â€¢ Total seizure time: 64197.67 seconds\n",
      "  â€¢ Total background time (sampled): 131470.95 seconds\n",
      "  â€¢ Background-to-Seizure ratio: 2.05:1\n",
      "  â€¢ Seizure percentage: 32.81%\n",
      "  âš ï¸  WARNING: Highly imbalanced dataset - consider:\n",
      "    - Weighted loss functions\n",
      "    - Oversampling seizure events\n",
      "    - Focal loss or similar techniques\n",
      "\n",
      "ðŸ’¾ FILE SIZE ANALYSIS:\n",
      "  â€¢ CSV files:\n",
      "    - Mean: 0.00 MB\n",
      "    - Median: 0.00 MB\n",
      "    - Total: 7.41 MB\n",
      "    - Range: 0.00-0.07 MB\n",
      "  â€¢ CSV_BI files:\n",
      "    - Mean: 0.00 MB\n",
      "    - Median: 0.00 MB\n",
      "    - Total: 1.53 MB\n",
      "    - Range: 0.00-0.00 MB\n",
      "  â€¢ EDF files:\n",
      "    - Mean: 10.96 MB\n",
      "    - Median: 7.64 MB\n",
      "    - Total: 80709.01 MB\n",
      "    - Range: 0.02-749.33 MB\n",
      "\n",
      "ðŸŽ›ï¸ RECORDING METADATA:\n",
      "  â€¢ Sampling rates:\n",
      "    - 256 Hz: 117 recordings (78.0%)\n",
      "    - 250 Hz: 18 recordings (12.0%)\n",
      "    - 400 Hz: 9 recordings (6.0%)\n",
      "    - 512 Hz: 3 recordings (2.0%)\n",
      "    - 1000 Hz: 3 recordings (2.0%)\n",
      "  â€¢ Channels per recording:\n",
      "    - Mean: 31.5\n",
      "    - Median: 32\n",
      "    - Range: 18-41\n",
      "  â€¢ Recording durations:\n",
      "    - Mean: 771.43 seconds (12.86 minutes)\n",
      "    - Median: 538.50 seconds\n",
      "    - Range: 1.00-18855.00 seconds\n",
      "\n",
      "ðŸ“¡ CHANNEL NAMING PATTERNS:\n",
      "  â€¢ Common channel prefixes (sample):\n",
      "    - EEG FP1: 4\n",
      "    - EEG FP2: 4\n",
      "    - EEG F3: 4\n",
      "    - EEG F4: 4\n",
      "    - EEG C3: 4\n",
      "    - EEG C4: 4\n",
      "    - EEG P3: 4\n",
      "    - EEG P4: 4\n",
      "    - EEG O1: 4\n",
      "    - EEG O2: 4\n",
      "\n",
      "ðŸ“‚ DATASET STRUCTURE:\n",
      "  â€¢ Maximum directory depth: 3\n",
      "  â€¢ Average directory depth: 2.2\n",
      "  â€¢ Train/Eval/Dev split:\n",
      "    - Train: 4,667 recordings (63.4%)\n",
      "    - Eval: 865 recordings (11.7%)\n",
      "    - Dev: 1,832 recordings (24.9%)\n",
      "\n",
      "ðŸŽ¯ DATA QUALITY INDICATORS:\n",
      "  â€¢ Annotation coverage: 100.0% (CSV/EDF ratio)\n",
      "  â€¢ Bi-annotation coverage: 100.0% (CSV_BI/EDF ratio)\n",
      "  â€¢ Patients with multiple sessions: 410\n",
      "  â€¢ Patients with >5 recordings: 270\n",
      "  â€¢ Average confidence score: 1.00 (from parsed data)\n",
      "\n",
      "================================================================================\n",
      "ðŸ’¡ RECOMMENDATIONS FOR SEIZURE PREDICTION PROJECT\n",
      "================================================================================\n",
      "\n",
      "ðŸ”¬ DATA PREPROCESSING:\n",
      "  1. Standardize sampling rates (resample to most common rate)\n",
      "  2. Handle channel naming inconsistencies\n",
      "  3. Apply bandpass filtering (typically 0.5-70 Hz for EEG)\n",
      "  4. Remove artifacts (eye blinks, muscle activity, electrical noise)\n",
      "  5. Normalize signal amplitude across recordings\n",
      "\n",
      "ðŸŽ¯ MODELING STRATEGY:\n",
      "  1. Address severe class imbalance:\n",
      "     - Use weighted cross-entropy loss\n",
      "     - Consider SMOTE or other oversampling techniques\n",
      "     - Try focal loss for hard examples\n",
      "  2. Feature engineering:\n",
      "     - Extract time-frequency features (wavelets, STFT)\n",
      "     - Compute spectral power in different frequency bands\n",
      "     - Calculate connectivity metrics between channels\n",
      "  3. Consider patient-specific models or transfer learning\n",
      "  4. Use sliding window approach for prediction\n",
      "\n",
      "ðŸ“Š DATA SPLIT RECOMMENDATIONS:\n",
      "  1. Patient-level split (avoid data leakage)\n",
      "  2. Stratify by seizure type if possible\n",
      "  3. Consider temporal validation (earlier sessions for training)\n",
      "  4. Suggested split: 70% train, 15% validation, 15% test\n",
      "\n",
      "ðŸ§  PREDICTION HORIZONS:\n",
      "  â€¢ Median seizure duration: 79.6s\n",
      "  â€¢ Suggested prediction horizons:\n",
      "    - Short-term: 5-30 seconds before onset\n",
      "    - Medium-term: 30-180 seconds before onset\n",
      "    - Long-term: 3-10 minutes before onset\n",
      "\n",
      "ðŸ“ˆ EVALUATION METRICS:\n",
      "  1. Sensitivity/Recall (critical for seizure detection)\n",
      "  2. False Positive Rate (per hour)\n",
      "  3. Area Under ROC Curve (AUC-ROC)\n",
      "  4. Precision-Recall curve (better for imbalanced data)\n",
      "  5. Time to detection (latency)\n",
      "\n",
      "ðŸ”§ TECHNICAL CONSIDERATIONS:\n",
      "  1. GPU memory constraints with long recordings\n",
      "  2. Real-time processing requirements\n",
      "  3. Multi-channel spatial relationships\n",
      "  4. Temporal dependencies (consider RNNs, LSTMs, or Transformers)\n",
      "  5. Interpretability for clinical validation\n",
      "\n",
      "ðŸ“š SUGGESTED ARCHITECTURES:\n",
      "  1. 1D CNN + LSTM for temporal modeling\n",
      "  2. Graph Neural Networks (channels as nodes)\n",
      "  3. Transformer-based models (EEGFormer, BIOTFormer)\n",
      "  4. ResNet/EfficientNet adapted for 1D signals\n",
      "  5. Ensemble methods combining multiple approaches\n",
      "\n",
      "âœ… NEXT STEPS:\n",
      "  1. âœ“ Dataset exploration complete\n",
      "  2. â†’ Load and visualize raw EEG signals (MNE-Python)\n",
      "  3. â†’ Implement preprocessing pipeline\n",
      "  4. â†’ Extract features and create training dataset\n",
      "  5. â†’ Build baseline model\n",
      "  6. â†’ Iterate and optimize\n",
      "\n",
      "================================================================================\n",
      "Analysis complete! Generated 15 comprehensive visualizations.\n",
      "Output saved to: tuh_tusz_comprehensive_analysis.png\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "\n",
    "# ========================= CONFIGURATION =========================\n",
    "DATASET_PATH = \"edf/\"  # UPDATE THIS PATH\n",
    "# =================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TUH TUSZ SEIZURE CORPUS - COMPREHENSIVE DATASET ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize data structures\n",
    "dataset_stats = {\n",
    "    'total_files': 0,\n",
    "    'edf_files': [],\n",
    "    'csv_files': [],\n",
    "    'csv_bi_files': [],\n",
    "    'patients': {},  # patient_id -> {sessions, recordings, demographics}\n",
    "    'sessions': defaultdict(list),\n",
    "    'file_sizes': defaultdict(list),\n",
    "    'directory_depth': [],\n",
    "    'train_eval_split': {'train': 0, 'eval': 0, 'dev': 0},\n",
    "}\n",
    "\n",
    "# Seizure-specific data\n",
    "seizure_data = {\n",
    "    'channel_annotations': defaultdict(list),  # per-channel annotations\n",
    "    'seizure_events': [],  # bi-channel seizure events (TERM)\n",
    "    'seizure_durations': [],\n",
    "    'seizure_types': [],\n",
    "    'background_durations': [],\n",
    "    'channels_involved': defaultdict(int),\n",
    "    'recording_durations': [],\n",
    "}\n",
    "\n",
    "# ==================== DATASET STRUCTURE EXPLORATION ====================\n",
    "print(\"\\n[1/7] Scanning dataset structure...\")\n",
    "\n",
    "for root, dirs, files in os.walk(DATASET_PATH):\n",
    "    depth = root.replace(DATASET_PATH, '').count(os.sep)\n",
    "    dataset_stats['directory_depth'].append(depth)\n",
    "   \n",
    "    # Determine train/eval/dev split\n",
    "    if 'train' in root.lower():\n",
    "        dataset_stats['train_eval_split']['train'] += len([f for f in files if f.endswith('.edf')])\n",
    "    elif 'eval' in root.lower():\n",
    "        dataset_stats['train_eval_split']['eval'] += len([f for f in files if f.endswith('.edf')])\n",
    "    elif 'dev' in root.lower():\n",
    "        dataset_stats['train_eval_split']['dev'] += len([f for f in files if f.endswith('.edf')])\n",
    "   \n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "       \n",
    "        dataset_stats['total_files'] += 1\n",
    "       \n",
    "        # Extract patient and session info from path\n",
    "        path_parts = Path(file_path).parts\n",
    "        patient_id = None\n",
    "        session_id = None\n",
    "       \n",
    "        # Find patient ID (format: aaaaaaaa)\n",
    "        for part in path_parts:\n",
    "            if re.match(r'^[a-z]{8}$', part):\n",
    "                patient_id = part\n",
    "            elif re.match(r'.*_s\\d{3}', part):\n",
    "                session_id = part\n",
    "       \n",
    "        if file.endswith('.edf'):\n",
    "            dataset_stats['edf_files'].append(file_path)\n",
    "            dataset_stats['file_sizes']['edf'].append(file_size)\n",
    "           \n",
    "            if patient_id:\n",
    "                if patient_id not in dataset_stats['patients']:\n",
    "                    dataset_stats['patients'][patient_id] = {\n",
    "                        'sessions': set(),\n",
    "                        'recordings': [],\n",
    "                        'age': None,\n",
    "                        'gender': None\n",
    "                    }\n",
    "                dataset_stats['patients'][patient_id]['recordings'].append(file_path)\n",
    "                if session_id:\n",
    "                    dataset_stats['patients'][patient_id]['sessions'].add(session_id)\n",
    "               \n",
    "        elif file.endswith('.csv_bi'):\n",
    "            dataset_stats['csv_bi_files'].append(file_path)\n",
    "            dataset_stats['file_sizes']['csv_bi'].append(file_size)\n",
    "           \n",
    "        elif file.endswith('.csv') and not file.endswith('.csv_bi'):\n",
    "            dataset_stats['csv_files'].append(file_path)\n",
    "            dataset_stats['file_sizes']['csv'].append(file_size)\n",
    "\n",
    "print(f\"âœ“ Scan complete: {dataset_stats['total_files']} files found\")\n",
    "print(f\"  - Patients: {len(dataset_stats['patients'])}\")\n",
    "print(f\"  - EDF files: {len(dataset_stats['edf_files'])}\")\n",
    "print(f\"  - CSV files: {len(dataset_stats['csv_files'])}\")\n",
    "print(f\"  - CSV_BI files: {len(dataset_stats['csv_bi_files'])}\")\n",
    "\n",
    "# ==================== PARSE CSV ANNOTATIONS ====================\n",
    "print(\"\\n[2/7] Parsing channel-level annotations (CSV files)...\")\n",
    "\n",
    "sample_size = min(200, len(dataset_stats['csv_files']))\n",
    "sampled_csvs = np.random.choice(dataset_stats['csv_files'], sample_size, replace=False) if dataset_stats['csv_files'] else []\n",
    "\n",
    "for csv_file in sampled_csvs:\n",
    "    try:\n",
    "        # Read CSV, skipping comment lines\n",
    "        df = pd.read_csv(csv_file, comment='#')\n",
    "       \n",
    "        if not df.empty and 'label' in df.columns:\n",
    "            # Extract recording duration from comments\n",
    "            with open(csv_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    if 'duration' in line:\n",
    "                        match = re.search(r'duration = ([\\d.]+)', line)\n",
    "                        if match:\n",
    "                            seizure_data['recording_durations'].append(float(match.group(1)))\n",
    "                            break\n",
    "           \n",
    "            # Process annotations\n",
    "            for _, row in df.iterrows():\n",
    "                label = row['label'].strip() if pd.notna(row['label']) else 'unknown'\n",
    "                channel = row['channel'] if pd.notna(row['channel']) else 'unknown'\n",
    "                duration = row['stop_time'] - row['start_time'] if pd.notna(row['start_time']) and pd.notna(row['stop_time']) else 0\n",
    "               \n",
    "                seizure_data['channel_annotations'][label].append({\n",
    "                    'channel': channel,\n",
    "                    'duration': duration,\n",
    "                    'start': row['start_time'] if pd.notna(row['start_time']) else 0,\n",
    "                    'stop': row['stop_time'] if pd.notna(row['stop_time']) else 0\n",
    "                })\n",
    "               \n",
    "                if label != 'bckg':  # Not background\n",
    "                    seizure_data['seizure_types'].append(label)\n",
    "                    seizure_data['channels_involved'][channel] += 1\n",
    "                else:\n",
    "                    seizure_data['background_durations'].append(duration)\n",
    "                   \n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f\"âœ“ Parsed {sample_size} CSV files\")\n",
    "print(f\"  - Unique seizure types: {len(set(seizure_data['seizure_types']))}\")\n",
    "print(f\"  - Channels with seizure activity: {len(seizure_data['channels_involved'])}\")\n",
    "\n",
    "# ==================== PARSE CSV_BI (SEIZURE EVENTS) ====================\n",
    "print(\"\\n[3/7] Parsing seizure events (CSV_BI files)...\")\n",
    "\n",
    "sample_size_bi = min(200, len(dataset_stats['csv_bi_files']))\n",
    "sampled_csv_bi = np.random.choice(dataset_stats['csv_bi_files'], sample_size_bi, replace=False) if dataset_stats['csv_bi_files'] else []\n",
    "\n",
    "for csv_bi_file in sampled_csv_bi:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_bi_file, comment='#')\n",
    "       \n",
    "        if not df.empty and 'label' in df.columns:\n",
    "            for _, row in df.iterrows():\n",
    "                if row['channel'] == 'TERM':  # Terminal event (overall seizure)\n",
    "                    duration = row['stop_time'] - row['start_time']\n",
    "                    seizure_data['seizure_events'].append({\n",
    "                        'start': row['start_time'],\n",
    "                        'stop': row['stop_time'],\n",
    "                        'duration': duration,\n",
    "                        'label': row['label'],\n",
    "                        'file': os.path.basename(csv_bi_file)\n",
    "                    })\n",
    "                    seizure_data['seizure_durations'].append(duration)\n",
    "                   \n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f\"âœ“ Parsed {sample_size_bi} CSV_BI files\")\n",
    "print(f\"  - Total seizure events detected: {len(seizure_data['seizure_events'])}\")\n",
    "\n",
    "# ==================== PARSE EDF HEADERS (SAMPLE) ====================\n",
    "print(\"\\n[4/7] Analyzing EDF file metadata...\")\n",
    "\n",
    "edf_metadata = {\n",
    "    'sampling_rates': [],\n",
    "    'num_channels': [],\n",
    "    'channel_names': [],\n",
    "    'recording_durations_edf': [],\n",
    "    'patient_ages': [],\n",
    "    'patient_genders': []\n",
    "}\n",
    "\n",
    "sample_edf = min(50, len(dataset_stats['edf_files']))\n",
    "sampled_edfs = np.random.choice(dataset_stats['edf_files'], sample_edf, replace=False) if dataset_stats['edf_files'] else []\n",
    "\n",
    "for edf_file in sampled_edfs:\n",
    "    try:\n",
    "        with open(edf_file, 'rb') as f:\n",
    "            header = f.read(256).decode('ascii', errors='ignore')\n",
    "           \n",
    "            # Extract patient info\n",
    "            patient_info = header[8:88].strip()\n",
    "            if 'Age:' in patient_info:\n",
    "                age_match = re.search(r'Age:(\\d+)', patient_info)\n",
    "                if age_match:\n",
    "                    edf_metadata['patient_ages'].append(int(age_match.group(1)))\n",
    "           \n",
    "            gender = header[8:9]\n",
    "            if gender in ['M', 'F']:\n",
    "                edf_metadata['patient_genders'].append(gender)\n",
    "           \n",
    "            # Number of signals\n",
    "            num_signals = int(header[252:256].strip())\n",
    "            edf_metadata['num_channels'].append(num_signals)\n",
    "           \n",
    "            # Read extended header for channel info\n",
    "            extended_header_size = num_signals * 256\n",
    "            extended = f.read(extended_header_size).decode('ascii', errors='ignore')\n",
    "           \n",
    "            # Extract channel names (16 chars each)\n",
    "            for i in range(num_signals):\n",
    "                start = i * 16\n",
    "                end = start + 16\n",
    "                if start < len(extended):\n",
    "                    ch_name = extended[start:end].strip()\n",
    "                    if ch_name and 'EEG' in ch_name:\n",
    "                        edf_metadata['channel_names'].append(ch_name)\n",
    "           \n",
    "            # Extract sampling rate (last 8 chars of each signal's info)\n",
    "            samples_per_record_start = num_signals * 216\n",
    "            for i in range(min(3, num_signals)):  # Sample first 3 channels\n",
    "                start = samples_per_record_start + i * 8\n",
    "                end = start + 8\n",
    "                if end <= len(extended):\n",
    "                    try:\n",
    "                        samp_rate = int(extended[start:end].strip())\n",
    "                        if 50 <= samp_rate <= 1000:  # Reasonable EEG sampling rates\n",
    "                            edf_metadata['sampling_rates'].append(samp_rate)\n",
    "                    except:\n",
    "                        pass\n",
    "                       \n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f\"âœ“ Analyzed {sample_edf} EDF files\")\n",
    "\n",
    "# ==================== COMPUTE STATISTICS ====================\n",
    "print(\"\\n[5/7] Computing comprehensive statistics...\")\n",
    "\n",
    "stats_summary = {\n",
    "    'Total Files': dataset_stats['total_files'],\n",
    "    'EDF Files (EEG recordings)': len(dataset_stats['edf_files']),\n",
    "    'CSV Files (channel annotations)': len(dataset_stats['csv_files']),\n",
    "    'CSV_BI Files (seizure events)': len(dataset_stats['csv_bi_files']),\n",
    "    'Unique Patients': len(dataset_stats['patients']),\n",
    "    'Total Sessions': sum(len(p['sessions']) for p in dataset_stats['patients'].values()),\n",
    "    'Recordings in Train': dataset_stats['train_eval_split']['train'],\n",
    "    'Recordings in Eval': dataset_stats['train_eval_split']['eval'],\n",
    "    'Recordings in Dev': dataset_stats['train_eval_split']['dev'],\n",
    "}\n",
    "\n",
    "# Patient statistics\n",
    "recordings_per_patient = [len(p['recordings']) for p in dataset_stats['patients'].values()]\n",
    "sessions_per_patient = [len(p['sessions']) for p in dataset_stats['patients'].values()]\n",
    "\n",
    "# Seizure statistics\n",
    "seizure_type_counts = Counter(seizure_data['seizure_types'])\n",
    "channel_counts = Counter([ann['channel'] for anns in seizure_data['channel_annotations'].values() for ann in anns])\n",
    "\n",
    "# ==================== VISUALIZATION ====================\n",
    "print(\"\\n[6/7] Generating comprehensive visualizations...\")\n",
    "\n",
    "fig = plt.figure(figsize=(24, 28))\n",
    "\n",
    "# 1. Dataset Composition\n",
    "ax1 = plt.subplot(5, 3, 1)\n",
    "file_types = ['EDF\\nRecordings', 'CSV\\nChannel\\nAnnotations', 'CSV_BI\\nSeizure\\nEvents']\n",
    "file_counts = [len(dataset_stats['edf_files']), len(dataset_stats['csv_files']), len(dataset_stats['csv_bi_files'])]\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "bars = ax1.bar(file_types, file_counts, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Dataset File Composition', fontsize=14, fontweight='bold', pad=20)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height):,}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 2. Train/Eval/Dev Split\n",
    "ax2 = plt.subplot(5, 3, 2)\n",
    "split_data = [v for k, v in dataset_stats['train_eval_split'].items() if v > 0]\n",
    "split_labels = [k.upper() for k, v in dataset_stats['train_eval_split'].items() if v > 0]\n",
    "if split_data:\n",
    "    wedges, texts, autotexts = ax2.pie(split_data, labels=split_labels, autopct='%1.1f%%',\n",
    "                                         colors=['#3498db', '#e74c3c', '#f39c12'], startangle=90)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "        autotext.set_fontsize(12)\n",
    "    ax2.set_title('Train/Eval/Dev Split', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# 3. Recordings per Patient\n",
    "ax3 = plt.subplot(5, 3, 3)\n",
    "ax3.hist(recordings_per_patient, bins=30, color='#9b59b6', alpha=0.7, edgecolor='black')\n",
    "ax3.set_xlabel('Recordings per Patient', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Number of Patients', fontsize=12, fontweight='bold')\n",
    "ax3.set_title(f'Patient Recording Distribution\\n(Î¼={np.mean(recordings_per_patient):.1f}, Ïƒ={np.std(recordings_per_patient):.1f})',\n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "ax3.axvline(np.mean(recordings_per_patient), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "ax3.legend()\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# 4. Seizure Type Distribution\n",
    "ax4 = plt.subplot(5, 3, 4)\n",
    "if seizure_type_counts:\n",
    "    types = list(seizure_type_counts.keys())[:10]\n",
    "    counts = [seizure_type_counts[t] for t in types]\n",
    "    bars = ax4.barh(types, counts, color='#e67e22', alpha=0.7, edgecolor='black')\n",
    "    ax4.set_xlabel('Count', fontsize=12, fontweight='bold')\n",
    "    ax4.set_title('Seizure Types (Top 10)', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax4.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 5. Seizure Duration Distribution\n",
    "ax5 = plt.subplot(5, 3, 5)\n",
    "if seizure_data['seizure_durations']:\n",
    "    ax5.hist(seizure_data['seizure_durations'], bins=40, color='#c0392b', alpha=0.7, edgecolor='black')\n",
    "    ax5.set_xlabel('Duration (seconds)', fontsize=12, fontweight='bold')\n",
    "    ax5.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    mean_dur = np.mean(seizure_data['seizure_durations'])\n",
    "    median_dur = np.median(seizure_data['seizure_durations'])\n",
    "    ax5.set_title(f'Seizure Duration Distribution\\n(Î¼={mean_dur:.1f}s, median={median_dur:.1f}s)',\n",
    "                  fontsize=14, fontweight='bold', pad=20)\n",
    "    ax5.axvline(mean_dur, color='yellow', linestyle='--', linewidth=2, label='Mean')\n",
    "    ax5.axvline(median_dur, color='green', linestyle='--', linewidth=2, label='Median')\n",
    "    ax5.legend()\n",
    "    ax5.grid(alpha=0.3)\n",
    "\n",
    "# 6. Recording Duration Distribution\n",
    "ax6 = plt.subplot(5, 3, 6)\n",
    "if seizure_data['recording_durations']:\n",
    "    ax6.hist(seizure_data['recording_durations'], bins=40, color='#16a085', alpha=0.7, edgecolor='black')\n",
    "    ax6.set_xlabel('Duration (seconds)', fontsize=12, fontweight='bold')\n",
    "    ax6.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    mean_rec = np.mean(seizure_data['recording_durations'])\n",
    "    ax6.set_title(f'Recording Duration Distribution\\n(Î¼={mean_rec:.1f}s)',\n",
    "                  fontsize=14, fontweight='bold', pad=20)\n",
    "    ax6.axvline(mean_rec, color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    ax6.legend()\n",
    "    ax6.grid(alpha=0.3)\n",
    "\n",
    "# 7. Most Active Channels (Seizure Activity)\n",
    "ax7 = plt.subplot(5, 3, 7)\n",
    "if seizure_data['channels_involved']:\n",
    "    top_channels = sorted(seizure_data['channels_involved'].items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "    channels = [ch[0] for ch in top_channels]\n",
    "    counts = [ch[1] for ch in top_channels]\n",
    "    bars = ax7.barh(channels, counts, color='#8e44ad', alpha=0.7, edgecolor='black')\n",
    "    ax7.set_xlabel('Seizure Event Count', fontsize=12, fontweight='bold')\n",
    "    ax7.set_title('Top 15 Channels with Seizure Activity', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax7.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 8. Sampling Rate Distribution\n",
    "ax8 = plt.subplot(5, 3, 8)\n",
    "if edf_metadata['sampling_rates']:\n",
    "    sampling_rate_counts = Counter(edf_metadata['sampling_rates'])\n",
    "    rates = list(sampling_rate_counts.keys())\n",
    "    counts = list(sampling_rate_counts.values())\n",
    "    ax8.bar(rates, counts, color='#27ae60', alpha=0.7, edgecolor='black', width=20)\n",
    "    ax8.set_xlabel('Sampling Rate (Hz)', fontsize=12, fontweight='bold')\n",
    "    ax8.set_ylabel('Number of Recordings', fontsize=12, fontweight='bold')\n",
    "    ax8.set_title(f'Sampling Rate Distribution\\n(Most common: {max(sampling_rate_counts, key=sampling_rate_counts.get)} Hz)',\n",
    "                  fontsize=14, fontweight='bold', pad=20)\n",
    "    ax8.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 9. Number of Channels per Recording\n",
    "ax9 = plt.subplot(5, 3, 9)\n",
    "if edf_metadata['num_channels']:\n",
    "    ax9.hist(edf_metadata['num_channels'], bins=20, color='#2980b9', alpha=0.7, edgecolor='black')\n",
    "    ax9.set_xlabel('Number of Channels', fontsize=12, fontweight='bold')\n",
    "    ax9.set_ylabel('Number of Recordings', fontsize=12, fontweight='bold')\n",
    "    mean_ch = np.mean(edf_metadata['num_channels'])\n",
    "    ax9.set_title(f'Channel Count Distribution\\n(Î¼={mean_ch:.1f} channels)',\n",
    "                  fontsize=14, fontweight='bold', pad=20)\n",
    "    ax9.axvline(mean_ch, color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    ax9.legend()\n",
    "    ax9.grid(alpha=0.3)\n",
    "\n",
    "# 10. Patient Age Distribution\n",
    "ax10 = plt.subplot(5, 3, 10)\n",
    "if edf_metadata['patient_ages']:\n",
    "    ax10.hist(edf_metadata['patient_ages'], bins=30, color='#d35400', alpha=0.7, edgecolor='black')\n",
    "    ax10.set_xlabel('Age (years)', fontsize=12, fontweight='bold')\n",
    "    ax10.set_ylabel('Number of Patients', fontsize=12, fontweight='bold')\n",
    "    mean_age = np.mean(edf_metadata['patient_ages'])\n",
    "    ax10.set_title(f'Patient Age Distribution\\n(Î¼={mean_age:.1f} years, range={min(edf_metadata[\"patient_ages\"])}-{max(edf_metadata[\"patient_ages\"])})',\n",
    "                   fontsize=14, fontweight='bold', pad=20)\n",
    "    ax10.axvline(mean_age, color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    ax10.legend()\n",
    "    ax10.grid(alpha=0.3)\n",
    "\n",
    "# 11. Patient Gender Distribution\n",
    "ax11 = plt.subplot(5, 3, 11)\n",
    "if edf_metadata['patient_genders']:\n",
    "    gender_counts = Counter(edf_metadata['patient_genders'])\n",
    "    genders = list(gender_counts.keys())\n",
    "    counts = list(gender_counts.values())\n",
    "    colors_gender = ['#3498db' if g == 'M' else '#e91e63' for g in genders]\n",
    "    bars = ax11.bar(genders, counts, color=colors_gender, alpha=0.7, edgecolor='black')\n",
    "    ax11.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "    ax11.set_title('Patient Gender Distribution', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax11.grid(axis='y', alpha=0.3)\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax11.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{int(height)}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 12. Sessions per Patient\n",
    "ax12 = plt.subplot(5, 3, 12)\n",
    "if sessions_per_patient:\n",
    "    ax12.hist(sessions_per_patient, bins=20, color='#1abc9c', alpha=0.7, edgecolor='black')\n",
    "    ax12.set_xlabel('Sessions per Patient', fontsize=12, fontweight='bold')\n",
    "    ax12.set_ylabel('Number of Patients', fontsize=12, fontweight='bold')\n",
    "    mean_sess = np.mean(sessions_per_patient)\n",
    "    ax12.set_title(f'Patient Session Distribution\\n(Î¼={mean_sess:.1f} sessions)',\n",
    "                   fontsize=14, fontweight='bold', pad=20)\n",
    "    ax12.axvline(mean_sess, color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    ax12.legend()\n",
    "    ax12.grid(alpha=0.3)\n",
    "\n",
    "# 13. File Size Distribution by Type\n",
    "ax13 = plt.subplot(5, 3, 13)\n",
    "file_size_data = []\n",
    "file_size_labels = []\n",
    "for ftype, sizes in dataset_stats['file_sizes'].items():\n",
    "    if sizes:\n",
    "        file_size_data.extend(sizes)\n",
    "        file_size_labels.extend([ftype.upper()] * len(sizes))\n",
    "\n",
    "if file_size_data:\n",
    "    import pandas as pd\n",
    "    size_df = pd.DataFrame({'Size (MB)': file_size_data, 'Type': file_size_labels})\n",
    "    sns.boxplot(data=size_df, x='Type', y='Size (MB)', ax=ax13, palette='Set2')\n",
    "    ax13.set_title('File Size Distribution by Type', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax13.set_ylabel('Size (MB)', fontsize=12, fontweight='bold')\n",
    "    ax13.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 14. Seizure vs Background Time Ratio\n",
    "ax14 = plt.subplot(5, 3, 14)\n",
    "if seizure_data['seizure_durations'] and seizure_data['background_durations']:\n",
    "    total_seizure = sum(seizure_data['seizure_durations'])\n",
    "    total_background = sum(seizure_data['background_durations'][:len(seizure_data['seizure_durations'])])  # Match sample size\n",
    "    data = [total_seizure, total_background]\n",
    "    labels = [f'Seizure\\n({total_seizure:.0f}s)', f'Background\\n({total_background:.0f}s)']\n",
    "    colors_pie = ['#e74c3c', '#95a5a6']\n",
    "    wedges, texts, autotexts = ax14.pie(data, labels=labels, autopct='%1.1f%%',\n",
    "                                         colors=colors_pie, startangle=90)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "        autotext.set_fontsize(11)\n",
    "    ax14.set_title('Seizure vs Background Time\\n(Class Balance)', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# 15. Dataset Quality Metrics Table\n",
    "ax15 = plt.subplot(5, 3, 15)\n",
    "ax15.axis('tight')\n",
    "ax15.axis('off')\n",
    "\n",
    "quality_metrics = [\n",
    "    ['Total Patients', f\"{len(dataset_stats['patients']):,}\"],\n",
    "    ['Total Recordings', f\"{len(dataset_stats['edf_files']):,}\"],\n",
    "    ['Avg Recordings/Patient', f\"{np.mean(recordings_per_patient):.1f}\"],\n",
    "    ['Seizure Events Detected', f\"{len(seizure_data['seizure_events']):,}\"],\n",
    "    ['Unique Seizure Types', f\"{len(seizure_type_counts)}\"],\n",
    "    ['Avg Seizure Duration', f\"{np.mean(seizure_data['seizure_durations']):.1f}s\" if seizure_data['seizure_durations'] else \"N/A\"],\n",
    "    ['Avg Recording Duration', f\"{np.mean(seizure_data['recording_durations']):.1f}s\" if seizure_data['recording_durations'] else \"N/A\"],\n",
    "    ['Most Common Samp. Rate', f\"{max(Counter(edf_metadata['sampling_rates']), key=Counter(edf_metadata['sampling_rates']).get)} Hz\" if edf_metadata['sampling_rates'] else \"N/A\"],\n",
    "]\n",
    "\n",
    "table = ax15.table(cellText=quality_metrics, colWidths=[0.6, 0.4],\n",
    "                  cellLoc='left', loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "for i in range(len(quality_metrics)):\n",
    "    cell = table[(i, 0)]\n",
    "    cell.set_facecolor('#ecf0f1' if i % 2 == 0 else 'white')\n",
    "    cell.set_text_props(weight='bold')\n",
    "    cell = table[(i, 1)]\n",
    "    cell.set_facecolor('#ecf0f1' if i % 2 == 0 else 'white')\n",
    "ax15.set_title('Key Dataset Metrics', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.savefig('viz_1_plots/tuh_tusz_comprehensive_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ“ Visualizations saved to 'tuh_tusz_comprehensive_analysis.png'\")\n",
    "\n",
    "# ==================== DETAILED REPORT ====================\n",
    "print(\"\\n[7/7] Generating detailed report...\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED TUH TUSZ DATASET REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“Š DATASET OVERVIEW:\")\n",
    "for key, value in stats_summary.items():\n",
    "    print(f\"  â€¢ {key}: {value:,}\" if isinstance(value, int) else f\"  â€¢ {key}: {value}\")\n",
    "\n",
    "print(f\"\\nðŸ¥ PATIENT STATISTICS:\")\n",
    "print(f\"  â€¢ Total unique patients: {len(dataset_stats['patients']):,}\")\n",
    "print(f\"  â€¢ Total sessions: {stats_summary['Total Sessions']:,}\")\n",
    "print(f\"  â€¢ Recordings per patient:\")\n",
    "print(f\"    - Mean: {np.mean(recordings_per_patient):.2f} Â± {np.std(recordings_per_patient):.2f}\")\n",
    "print(f\"    - Median: {np.median(recordings_per_patient):.0f}\")\n",
    "print(f\"    - Range: {min(recordings_per_patient)}-{max(recordings_per_patient)}\")\n",
    "print(f\"  â€¢ Sessions per patient:\")\n",
    "print(f\"    - Mean: {np.mean(sessions_per_patient):.2f} Â± {np.std(sessions_per_patient):.2f}\")\n",
    "print(f\"    - Median: {np.median(sessions_per_patient):.0f}\")\n",
    "\n",
    "if edf_metadata['patient_ages']:\n",
    "    print(f\"  â€¢ Patient age:\")\n",
    "    print(f\"    - Mean: {np.mean(edf_metadata['patient_ages']):.1f} years\")\n",
    "    print(f\"    - Range: {min(edf_metadata['patient_ages'])}-{max(edf_metadata['patient_ages'])} years\")\n",
    "\n",
    "if edf_metadata['patient_genders']:\n",
    "    gender_dist = Counter(edf_metadata['patient_genders'])\n",
    "    print(f\"  â€¢ Gender distribution:\")\n",
    "    for gender, count in gender_dist.items():\n",
    "        print(f\"    - {gender}: {count} ({count/len(edf_metadata['patient_genders'])*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ§  SEIZURE ANNOTATION STATISTICS:\")\n",
    "print(f\"  â€¢ Total seizure events (from CSV_BI): {len(seizure_data['seizure_events']):,}\")\n",
    "print(f\"  â€¢ Unique seizure types: {len(seizure_type_counts)}\")\n",
    "print(f\"  â€¢ Most common seizure types:\")\n",
    "if seizure_type_counts:\n",
    "    for stype, count in seizure_type_counts.most_common(5):\n",
    "        print(f\"    - {stype}: {count} ({count/sum(seizure_type_counts.values())*100:.1f}%)\")\n",
    "\n",
    "if seizure_data['seizure_durations']:\n",
    "    print(f\"  â€¢ Seizure duration statistics:\")\n",
    "    print(f\"    - Mean: {np.mean(seizure_data['seizure_durations']):.2f} seconds\")\n",
    "    print(f\"    - Median: {np.median(seizure_data['seizure_durations']):.2f} seconds\")\n",
    "    print(f\"    - Std: {np.std(seizure_data['seizure_durations']):.2f} seconds\")\n",
    "    print(f\"    - Range: {min(seizure_data['seizure_durations']):.2f}-{max(seizure_data['seizure_durations']):.2f} seconds\")\n",
    "    print(f\"    - Percentiles:\")\n",
    "    print(f\"      * 25th: {np.percentile(seizure_data['seizure_durations'], 25):.2f}s\")\n",
    "    print(f\"      * 75th: {np.percentile(seizure_data['seizure_durations'], 75):.2f}s\")\n",
    "    print(f\"      * 90th: {np.percentile(seizure_data['seizure_durations'], 90):.2f}s\")\n",
    "\n",
    "print(f\"\\nðŸ“ CHANNEL ANNOTATION ANALYSIS:\")\n",
    "print(f\"  â€¢ Total channels with annotations: {len(seizure_data['channels_involved'])}\")\n",
    "print(f\"  â€¢ Most active channels (seizure activity):\")\n",
    "if seizure_data['channels_involved']:\n",
    "    top_channels = sorted(seizure_data['channels_involved'].items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    for channel, count in top_channels:\n",
    "        print(f\"    - {channel}: {count} events\")\n",
    "\n",
    "print(f\"\\nâš–ï¸ CLASS BALANCE ANALYSIS:\")\n",
    "if seizure_data['seizure_durations'] and seizure_data['background_durations']:\n",
    "    total_seizure = sum(seizure_data['seizure_durations'])\n",
    "    total_background = sum(seizure_data['background_durations'][:len(seizure_data['seizure_durations'])])\n",
    "    ratio = total_background / total_seizure if total_seizure > 0 else 0\n",
    "    print(f\"  â€¢ Total seizure time: {total_seizure:.2f} seconds\")\n",
    "    print(f\"  â€¢ Total background time (sampled): {total_background:.2f} seconds\")\n",
    "    print(f\"  â€¢ Background-to-Seizure ratio: {ratio:.2f}:1\")\n",
    "    print(f\"  â€¢ Seizure percentage: {total_seizure/(total_seizure+total_background)*100:.2f}%\")\n",
    "    print(f\"  âš ï¸  WARNING: Highly imbalanced dataset - consider:\")\n",
    "    print(f\"    - Weighted loss functions\")\n",
    "    print(f\"    - Oversampling seizure events\")\n",
    "    print(f\"    - Focal loss or similar techniques\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ FILE SIZE ANALYSIS:\")\n",
    "for ftype, sizes in dataset_stats['file_sizes'].items():\n",
    "    if sizes:\n",
    "        print(f\"  â€¢ {ftype.upper()} files:\")\n",
    "        print(f\"    - Mean: {np.mean(sizes):.2f} MB\")\n",
    "        print(f\"    - Median: {np.median(sizes):.2f} MB\")\n",
    "        print(f\"    - Total: {sum(sizes):.2f} MB\")\n",
    "        print(f\"    - Range: {min(sizes):.2f}-{max(sizes):.2f} MB\")\n",
    "\n",
    "print(f\"\\nðŸŽ›ï¸ RECORDING METADATA:\")\n",
    "if edf_metadata['sampling_rates']:\n",
    "    rate_dist = Counter(edf_metadata['sampling_rates'])\n",
    "    print(f\"  â€¢ Sampling rates:\")\n",
    "    for rate, count in rate_dist.most_common():\n",
    "        print(f\"    - {rate} Hz: {count} recordings ({count/len(edf_metadata['sampling_rates'])*100:.1f}%)\")\n",
    "\n",
    "if edf_metadata['num_channels']:\n",
    "    print(f\"  â€¢ Channels per recording:\")\n",
    "    print(f\"    - Mean: {np.mean(edf_metadata['num_channels']):.1f}\")\n",
    "    print(f\"    - Median: {np.median(edf_metadata['num_channels']):.0f}\")\n",
    "    print(f\"    - Range: {min(edf_metadata['num_channels'])}-{max(edf_metadata['num_channels'])}\")\n",
    "\n",
    "if seizure_data['recording_durations']:\n",
    "    print(f\"  â€¢ Recording durations:\")\n",
    "    print(f\"    - Mean: {np.mean(seizure_data['recording_durations']):.2f} seconds ({np.mean(seizure_data['recording_durations'])/60:.2f} minutes)\")\n",
    "    print(f\"    - Median: {np.median(seizure_data['recording_durations']):.2f} seconds\")\n",
    "    print(f\"    - Range: {min(seizure_data['recording_durations']):.2f}-{max(seizure_data['recording_durations']):.2f} seconds\")\n",
    "\n",
    "if edf_metadata['channel_names']:\n",
    "    print(f\"\\nðŸ“¡ CHANNEL NAMING PATTERNS:\")\n",
    "    channel_name_patterns = Counter([ch.split('-')[0] if '-' in ch else ch for ch in edf_metadata['channel_names'][:100]])\n",
    "    print(f\"  â€¢ Common channel prefixes (sample):\")\n",
    "    for pattern, count in channel_name_patterns.most_common(10):\n",
    "        print(f\"    - {pattern}: {count}\")\n",
    "\n",
    "print(f\"\\nðŸ“‚ DATASET STRUCTURE:\")\n",
    "print(f\"  â€¢ Maximum directory depth: {max(dataset_stats['directory_depth'])}\")\n",
    "print(f\"  â€¢ Average directory depth: {np.mean(dataset_stats['directory_depth']):.1f}\")\n",
    "print(f\"  â€¢ Train/Eval/Dev split:\")\n",
    "print(f\"    - Train: {dataset_stats['train_eval_split']['train']:,} recordings ({dataset_stats['train_eval_split']['train']/len(dataset_stats['edf_files'])*100:.1f}%)\")\n",
    "print(f\"    - Eval: {dataset_stats['train_eval_split']['eval']:,} recordings ({dataset_stats['train_eval_split']['eval']/len(dataset_stats['edf_files'])*100:.1f}%)\")\n",
    "if dataset_stats['train_eval_split']['dev'] > 0:\n",
    "    print(f\"    - Dev: {dataset_stats['train_eval_split']['dev']:,} recordings ({dataset_stats['train_eval_split']['dev']/len(dataset_stats['edf_files'])*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ DATA QUALITY INDICATORS:\")\n",
    "annotation_coverage = len(dataset_stats['csv_files']) / len(dataset_stats['edf_files']) * 100 if dataset_stats['edf_files'] else 0\n",
    "print(f\"  â€¢ Annotation coverage: {annotation_coverage:.1f}% (CSV/EDF ratio)\")\n",
    "bi_annotation_coverage = len(dataset_stats['csv_bi_files']) / len(dataset_stats['edf_files']) * 100 if dataset_stats['edf_files'] else 0\n",
    "print(f\"  â€¢ Bi-annotation coverage: {bi_annotation_coverage:.1f}% (CSV_BI/EDF ratio)\")\n",
    "print(f\"  â€¢ Patients with multiple sessions: {sum(1 for p in dataset_stats['patients'].values() if len(p['sessions']) > 1)}\")\n",
    "print(f\"  â€¢ Patients with >5 recordings: {sum(1 for c in recordings_per_patient if c > 5)}\")\n",
    "print(f\"  â€¢ Average confidence score: {np.mean([1.0]*len(seizure_data['seizure_events'])):.2f} (from parsed data)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ’¡ RECOMMENDATIONS FOR SEIZURE PREDICTION PROJECT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ”¬ DATA PREPROCESSING:\")\n",
    "print(\"  1. Standardize sampling rates (resample to most common rate)\")\n",
    "print(\"  2. Handle channel naming inconsistencies\")\n",
    "print(\"  3. Apply bandpass filtering (typically 0.5-70 Hz for EEG)\")\n",
    "print(\"  4. Remove artifacts (eye blinks, muscle activity, electrical noise)\")\n",
    "print(\"  5. Normalize signal amplitude across recordings\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ MODELING STRATEGY:\")\n",
    "print(\"  1. Address severe class imbalance:\")\n",
    "print(\"     - Use weighted cross-entropy loss\")\n",
    "print(\"     - Consider SMOTE or other oversampling techniques\")\n",
    "print(\"     - Try focal loss for hard examples\")\n",
    "print(\"  2. Feature engineering:\")\n",
    "print(\"     - Extract time-frequency features (wavelets, STFT)\")\n",
    "print(\"     - Compute spectral power in different frequency bands\")\n",
    "print(\"     - Calculate connectivity metrics between channels\")\n",
    "print(\"  3. Consider patient-specific models or transfer learning\")\n",
    "print(\"  4. Use sliding window approach for prediction\")\n",
    "\n",
    "print(\"\\nðŸ“Š DATA SPLIT RECOMMENDATIONS:\")\n",
    "print(\"  1. Patient-level split (avoid data leakage)\")\n",
    "print(\"  2. Stratify by seizure type if possible\")\n",
    "print(\"  3. Consider temporal validation (earlier sessions for training)\")\n",
    "print(\"  4. Suggested split: 70% train, 15% validation, 15% test\")\n",
    "\n",
    "print(\"\\nðŸ§  PREDICTION HORIZONS:\")\n",
    "if seizure_data['seizure_durations']:\n",
    "    median_duration = np.median(seizure_data['seizure_durations'])\n",
    "    print(f\"  â€¢ Median seizure duration: {median_duration:.1f}s\")\n",
    "    print(f\"  â€¢ Suggested prediction horizons:\")\n",
    "    print(f\"    - Short-term: 5-30 seconds before onset\")\n",
    "    print(f\"    - Medium-term: 30-180 seconds before onset\")\n",
    "    print(f\"    - Long-term: 3-10 minutes before onset\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ EVALUATION METRICS:\")\n",
    "print(\"  1. Sensitivity/Recall (critical for seizure detection)\")\n",
    "print(\"  2. False Positive Rate (per hour)\")\n",
    "print(\"  3. Area Under ROC Curve (AUC-ROC)\")\n",
    "print(\"  4. Precision-Recall curve (better for imbalanced data)\")\n",
    "print(\"  5. Time to detection (latency)\")\n",
    "\n",
    "print(\"\\nðŸ”§ TECHNICAL CONSIDERATIONS:\")\n",
    "print(\"  1. GPU memory constraints with long recordings\")\n",
    "print(\"  2. Real-time processing requirements\")\n",
    "print(\"  3. Multi-channel spatial relationships\")\n",
    "print(\"  4. Temporal dependencies (consider RNNs, LSTMs, or Transformers)\")\n",
    "print(\"  5. Interpretability for clinical validation\")\n",
    "\n",
    "print(\"\\nðŸ“š SUGGESTED ARCHITECTURES:\")\n",
    "print(\"  1. 1D CNN + LSTM for temporal modeling\")\n",
    "print(\"  2. Graph Neural Networks (channels as nodes)\")\n",
    "print(\"  3. Transformer-based models (EEGFormer, BIOTFormer)\")\n",
    "print(\"  4. ResNet/EfficientNet adapted for 1D signals\")\n",
    "print(\"  5. Ensemble methods combining multiple approaches\")\n",
    "\n",
    "print(\"\\nâœ… NEXT STEPS:\")\n",
    "print(\"  1. âœ“ Dataset exploration complete\")\n",
    "print(\"  2. â†’ Load and visualize raw EEG signals (MNE-Python)\")\n",
    "print(\"  3. â†’ Implement preprocessing pipeline\")\n",
    "print(\"  4. â†’ Extract features and create training dataset\")\n",
    "print(\"  5. â†’ Build baseline model\")\n",
    "print(\"  6. â†’ Iterate and optimize\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Analysis complete! Generated 15 comprehensive visualizations.\")\n",
    "print(\"Output saved to: tuh_tusz_comprehensive_analysis.png\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5f1f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TUH TUSZ SEIZURE CORPUS - COMPREHENSIVE DATASET ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "[1/7] Scanning dataset structure...\n",
      "âœ“ Scan complete: 22092 files found\n",
      "  - Patients: 675\n",
      "  - EDF files: 7364\n",
      "  - CSV files: 7364\n",
      "  - CSV_BI files: 7364\n",
      "\n",
      "[2/7] Parsing channel-level annotations (CSV files)...\n",
      "âœ“ Parsed 200 CSV files\n",
      "  - Unique seizure types: 4\n",
      "  - Channels with seizure activity: 22\n",
      "\n",
      "[3/7] Parsing seizure events (CSV_BI files)...\n",
      "âœ“ Parsed 200 CSV_BI files\n",
      "  - Total seizure events detected: 348\n",
      "\n",
      "[4/7] Analyzing EDF file metadata...\n",
      "âœ“ Analyzed 50 EDF files\n",
      "\n",
      "[5/7] Computing comprehensive statistics...\n",
      "\n",
      "[6/7] Generating comprehensive visualizations...\n",
      "  âœ“ Saved: 01_dataset_composition.png\n",
      "  âœ“ Saved: 02_train_eval_split.png\n",
      "  âœ“ Saved: 03_recordings_per_patient.png\n",
      "  âœ“ Saved: 04_seizure_types.png\n",
      "  âœ“ Saved: 05_seizure_duration.png\n",
      "  âœ“ Saved: 06_recording_duration.png\n",
      "  âœ“ Saved: 07_active_channels.png\n",
      "  âœ“ Saved: 08_sampling_rates.png\n",
      "  âœ“ Saved: 09_channel_count.png\n",
      "  âœ“ Saved: 10_patient_age.png\n",
      "  âœ“ Saved: 12_sessions_per_patient.png\n",
      "  âœ“ Saved: 13_file_sizes.png\n",
      "  âœ“ Saved: 14_class_balance.png\n",
      "  âœ“ Saved: 15_summary_metrics.png\n",
      "\n",
      "âœ“ All visualizations saved to 'viz_1_plots/'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# ========================= CONFIGURATION =========================\n",
    "DATASET_PATH = \"edf/\"  # UPDATE THIS PATH\n",
    "OUTPUT_DIR = \"viz_1_plots/\"\n",
    "# =================================================================\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TUH TUSZ SEIZURE CORPUS - COMPREHENSIVE DATASET ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize data structures\n",
    "dataset_stats = {\n",
    "    'total_files': 0,\n",
    "    'edf_files': [],\n",
    "    'csv_files': [],\n",
    "    'csv_bi_files': [],\n",
    "    'patients': {},  # patient_id -> {sessions, recordings, demographics}\n",
    "    'sessions': defaultdict(list),\n",
    "    'file_sizes': defaultdict(list),\n",
    "    'directory_depth': [],\n",
    "    'train_eval_split': {'train': 0, 'eval': 0, 'dev': 0},\n",
    "}\n",
    "\n",
    "# Seizure-specific data\n",
    "seizure_data = {\n",
    "    'channel_annotations': defaultdict(list),  # per-channel annotations\n",
    "    'seizure_events': [],  # bi-channel seizure events (TERM)\n",
    "    'seizure_durations': [],\n",
    "    'seizure_types': [],\n",
    "    'background_durations': [],\n",
    "    'channels_involved': defaultdict(int),\n",
    "    'recording_durations': [],\n",
    "}\n",
    "\n",
    "# ==================== DATASET STRUCTURE EXPLORATION ====================\n",
    "print(\"\\n[1/7] Scanning dataset structure...\")\n",
    "\n",
    "for root, dirs, files in os.walk(DATASET_PATH):\n",
    "    depth = root.replace(DATASET_PATH, '').count(os.sep)\n",
    "    dataset_stats['directory_depth'].append(depth)\n",
    "   \n",
    "    # Determine train/eval/dev split\n",
    "    root_lower = root.lower()\n",
    "    if 'train' in root_lower:\n",
    "        dataset_stats['train_eval_split']['train'] += len([f for f in files if f.endswith('.edf')])\n",
    "    elif 'eval' in root_lower:\n",
    "        dataset_stats['train_eval_split']['eval'] += len([f for f in files if f.endswith('.edf')])\n",
    "    elif 'dev' in root_lower:\n",
    "        dataset_stats['train_eval_split']['dev'] += len([f for f in files if f.endswith('.edf')])\n",
    "   \n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        \n",
    "        # Check if file exists and get size\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "            \n",
    "        file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "       \n",
    "        dataset_stats['total_files'] += 1\n",
    "       \n",
    "        # Extract patient and session info from path\n",
    "        path_parts = Path(file_path).parts\n",
    "        patient_id = None\n",
    "        session_id = None\n",
    "       \n",
    "        # Find patient ID (format: aaaaaaaa)\n",
    "        for part in path_parts:\n",
    "            if re.match(r'^[a-z]{8}$', part):\n",
    "                patient_id = part\n",
    "            elif re.match(r'.*_s\\d{3}', part):\n",
    "                session_id = part\n",
    "       \n",
    "        if file.endswith('.edf'):\n",
    "            dataset_stats['edf_files'].append(file_path)\n",
    "            dataset_stats['file_sizes']['edf'].append(file_size)\n",
    "           \n",
    "            if patient_id:\n",
    "                if patient_id not in dataset_stats['patients']:\n",
    "                    dataset_stats['patients'][patient_id] = {\n",
    "                        'sessions': set(),\n",
    "                        'recordings': [],\n",
    "                        'age': None,\n",
    "                        'gender': None\n",
    "                    }\n",
    "                dataset_stats['patients'][patient_id]['recordings'].append(file_path)\n",
    "                if session_id:\n",
    "                    dataset_stats['patients'][patient_id]['sessions'].add(session_id)\n",
    "               \n",
    "        elif file.endswith('.csv_bi'):\n",
    "            dataset_stats['csv_bi_files'].append(file_path)\n",
    "            dataset_stats['file_sizes']['csv_bi'].append(file_size)\n",
    "           \n",
    "        elif file.endswith('.csv') and not file.endswith('.csv_bi'):\n",
    "            dataset_stats['csv_files'].append(file_path)\n",
    "            dataset_stats['file_sizes']['csv'].append(file_size)\n",
    "\n",
    "print(f\"âœ“ Scan complete: {dataset_stats['total_files']} files found\")\n",
    "print(f\"  - Patients: {len(dataset_stats['patients'])}\")\n",
    "print(f\"  - EDF files: {len(dataset_stats['edf_files'])}\")\n",
    "print(f\"  - CSV files: {len(dataset_stats['csv_files'])}\")\n",
    "print(f\"  - CSV_BI files: {len(dataset_stats['csv_bi_files'])}\")\n",
    "\n",
    "# ==================== PARSE CSV ANNOTATIONS ====================\n",
    "print(\"\\n[2/7] Parsing channel-level annotations (CSV files)...\")\n",
    "\n",
    "sample_size = min(200, len(dataset_stats['csv_files']))\n",
    "sampled_csvs = np.random.choice(dataset_stats['csv_files'], sample_size, replace=False) if len(dataset_stats['csv_files']) > 0 else []\n",
    "\n",
    "for csv_file in sampled_csvs:\n",
    "    try:\n",
    "        # Read CSV, skipping comment lines\n",
    "        df = pd.read_csv(csv_file, comment='#')\n",
    "       \n",
    "        if not df.empty and 'label' in df.columns:\n",
    "            # Extract recording duration from comments\n",
    "            with open(csv_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    if 'duration' in line.lower():\n",
    "                        match = re.search(r'duration\\s*=\\s*([\\d.]+)', line, re.IGNORECASE)\n",
    "                        if match:\n",
    "                            seizure_data['recording_durations'].append(float(match.group(1)))\n",
    "                            break\n",
    "           \n",
    "            # Process annotations\n",
    "            for _, row in df.iterrows():\n",
    "                label = row['label'].strip() if pd.notna(row['label']) else 'unknown'\n",
    "                channel = str(row['channel']) if pd.notna(row['channel']) else 'unknown'\n",
    "                \n",
    "                # Calculate duration safely\n",
    "                start_time = row['start_time'] if pd.notna(row['start_time']) else 0\n",
    "                stop_time = row['stop_time'] if pd.notna(row['stop_time']) else 0\n",
    "                duration = stop_time - start_time if stop_time >= start_time else 0\n",
    "               \n",
    "                seizure_data['channel_annotations'][label].append({\n",
    "                    'channel': channel,\n",
    "                    'duration': duration,\n",
    "                    'start': start_time,\n",
    "                    'stop': stop_time\n",
    "                })\n",
    "               \n",
    "                if label != 'bckg':  # Not background\n",
    "                    seizure_data['seizure_types'].append(label)\n",
    "                    seizure_data['channels_involved'][channel] += 1\n",
    "                else:\n",
    "                    seizure_data['background_durations'].append(duration)\n",
    "                   \n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f\"âœ“ Parsed {len(sampled_csvs)} CSV files\")\n",
    "print(f\"  - Unique seizure types: {len(set(seizure_data['seizure_types']))}\")\n",
    "print(f\"  - Channels with seizure activity: {len(seizure_data['channels_involved'])}\")\n",
    "\n",
    "# ==================== PARSE CSV_BI (SEIZURE EVENTS) ====================\n",
    "print(\"\\n[3/7] Parsing seizure events (CSV_BI files)...\")\n",
    "\n",
    "sample_size_bi = min(200, len(dataset_stats['csv_bi_files']))\n",
    "sampled_csv_bi = np.random.choice(dataset_stats['csv_bi_files'], sample_size_bi, replace=False) if len(dataset_stats['csv_bi_files']) > 0 else []\n",
    "\n",
    "for csv_bi_file in sampled_csv_bi:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_bi_file, comment='#')\n",
    "       \n",
    "        if not df.empty and 'label' in df.columns and 'channel' in df.columns:\n",
    "            for _, row in df.iterrows():\n",
    "                if str(row['channel']).strip() == 'TERM':  # Terminal event (overall seizure)\n",
    "                    start_time = row['start_time'] if pd.notna(row['start_time']) else 0\n",
    "                    stop_time = row['stop_time'] if pd.notna(row['stop_time']) else 0\n",
    "                    duration = stop_time - start_time if stop_time >= start_time else 0\n",
    "                    \n",
    "                    if duration > 0:  # Only add valid durations\n",
    "                        seizure_data['seizure_events'].append({\n",
    "                            'start': start_time,\n",
    "                            'stop': stop_time,\n",
    "                            'duration': duration,\n",
    "                            'label': row['label'] if pd.notna(row['label']) else 'unknown',\n",
    "                            'file': os.path.basename(csv_bi_file)\n",
    "                        })\n",
    "                        seizure_data['seizure_durations'].append(duration)\n",
    "                   \n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f\"âœ“ Parsed {len(sampled_csv_bi)} CSV_BI files\")\n",
    "print(f\"  - Total seizure events detected: {len(seizure_data['seizure_events'])}\")\n",
    "\n",
    "# ==================== PARSE EDF HEADERS (SAMPLE) ====================\n",
    "print(\"\\n[4/7] Analyzing EDF file metadata...\")\n",
    "\n",
    "edf_metadata = {\n",
    "    'sampling_rates': [],\n",
    "    'num_channels': [],\n",
    "    'channel_names': [],\n",
    "    'recording_durations_edf': [],\n",
    "    'patient_ages': [],\n",
    "    'patient_genders': []\n",
    "}\n",
    "\n",
    "sample_edf = min(50, len(dataset_stats['edf_files']))\n",
    "sampled_edfs = np.random.choice(dataset_stats['edf_files'], sample_edf, replace=False) if len(dataset_stats['edf_files']) > 0 else []\n",
    "\n",
    "for edf_file in sampled_edfs:\n",
    "    try:\n",
    "        with open(edf_file, 'rb') as f:\n",
    "            header = f.read(256).decode('ascii', errors='ignore')\n",
    "           \n",
    "            # Extract patient info\n",
    "            patient_info = header[8:88].strip()\n",
    "            if 'Age:' in patient_info:\n",
    "                age_match = re.search(r'Age:(\\d+)', patient_info)\n",
    "                if age_match:\n",
    "                    edf_metadata['patient_ages'].append(int(age_match.group(1)))\n",
    "           \n",
    "            gender = header[8:9]\n",
    "            if gender in ['M', 'F']:\n",
    "                edf_metadata['patient_genders'].append(gender)\n",
    "           \n",
    "            # Number of signals\n",
    "            num_signals_str = header[252:256].strip()\n",
    "            if num_signals_str:\n",
    "                num_signals = int(num_signals_str)\n",
    "                edf_metadata['num_channels'].append(num_signals)\n",
    "           \n",
    "                # Read extended header for channel info\n",
    "                extended_header_size = num_signals * 256\n",
    "                extended = f.read(extended_header_size).decode('ascii', errors='ignore')\n",
    "           \n",
    "                # Extract channel names (16 chars each)\n",
    "                for i in range(num_signals):\n",
    "                    start = i * 16\n",
    "                    end = start + 16\n",
    "                    if start < len(extended):\n",
    "                        ch_name = extended[start:end].strip()\n",
    "                        if ch_name and 'EEG' in ch_name:\n",
    "                            edf_metadata['channel_names'].append(ch_name)\n",
    "           \n",
    "                # Extract sampling rate (last 8 chars of each signal's info)\n",
    "                samples_per_record_start = num_signals * 216\n",
    "                for i in range(min(3, num_signals)):  # Sample first 3 channels\n",
    "                    start = samples_per_record_start + i * 8\n",
    "                    end = start + 8\n",
    "                    if end <= len(extended):\n",
    "                        try:\n",
    "                            samp_rate_str = extended[start:end].strip()\n",
    "                            if samp_rate_str:\n",
    "                                samp_rate = int(samp_rate_str)\n",
    "                                if 50 <= samp_rate <= 1000:  # Reasonable EEG sampling rates\n",
    "                                    edf_metadata['sampling_rates'].append(samp_rate)\n",
    "                        except:\n",
    "                            pass\n",
    "                       \n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f\"âœ“ Analyzed {len(sampled_edfs)} EDF files\")\n",
    "\n",
    "# ==================== COMPUTE STATISTICS ====================\n",
    "print(\"\\n[5/7] Computing comprehensive statistics...\")\n",
    "\n",
    "stats_summary = {\n",
    "    'Total Files': dataset_stats['total_files'],\n",
    "    'EDF Files (EEG recordings)': len(dataset_stats['edf_files']),\n",
    "    'CSV Files (channel annotations)': len(dataset_stats['csv_files']),\n",
    "    'CSV_BI Files (seizure events)': len(dataset_stats['csv_bi_files']),\n",
    "    'Unique Patients': len(dataset_stats['patients']),\n",
    "    'Total Sessions': sum(len(p['sessions']) for p in dataset_stats['patients'].values()),\n",
    "    'Recordings in Train': dataset_stats['train_eval_split']['train'],\n",
    "    'Recordings in Eval': dataset_stats['train_eval_split']['eval'],\n",
    "    'Recordings in Dev': dataset_stats['train_eval_split']['dev'],\n",
    "}\n",
    "\n",
    "# Patient statistics\n",
    "recordings_per_patient = [len(p['recordings']) for p in dataset_stats['patients'].values()] if dataset_stats['patients'] else [0]\n",
    "sessions_per_patient = [len(p['sessions']) for p in dataset_stats['patients'].values()] if dataset_stats['patients'] else [0]\n",
    "\n",
    "# Seizure statistics\n",
    "seizure_type_counts = Counter(seizure_data['seizure_types'])\n",
    "channel_counts = Counter([ann['channel'] for anns in seizure_data['channel_annotations'].values() for ann in anns])\n",
    "\n",
    "# ==================== VISUALIZATION ====================\n",
    "print(\"\\n[6/7] Generating comprehensive visualizations...\")\n",
    "\n",
    "def save_plot(filename):\n",
    "    \"\"\"Helper function to save plots\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"  âœ“ Saved: {filename}\")\n",
    "\n",
    "# 1. Dataset Composition\n",
    "plt.figure(figsize=(10, 6))\n",
    "file_types = ['EDF\\nRecordings', 'CSV\\nChannel\\nAnnotations', 'CSV_BI\\nSeizure\\nEvents']\n",
    "file_counts = [len(dataset_stats['edf_files']), len(dataset_stats['csv_files']), len(dataset_stats['csv_bi_files'])]\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "bars = plt.bar(file_types, file_counts, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "plt.ylabel('Count', fontsize=12, fontweight='bold')\n",
    "plt.title('Dataset File Composition', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height):,}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "save_plot('01_dataset_composition.png')\n",
    "\n",
    "# 2. Train/Eval/Dev Split\n",
    "if any(dataset_stats['train_eval_split'].values()):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    split_data = [v for k, v in dataset_stats['train_eval_split'].items() if v > 0]\n",
    "    split_labels = [k.upper() for k, v in dataset_stats['train_eval_split'].items() if v > 0]\n",
    "    if split_data:\n",
    "        wedges, texts, autotexts = plt.pie(split_data, labels=split_labels, autopct='%1.1f%%',\n",
    "                                             colors=['#3498db', '#e74c3c', '#f39c12'], startangle=90)\n",
    "        for autotext in autotexts:\n",
    "            autotext.set_color('white')\n",
    "            autotext.set_fontweight('bold')\n",
    "            autotext.set_fontsize(12)\n",
    "        plt.title('Train/Eval/Dev Split', fontsize=14, fontweight='bold', pad=20)\n",
    "        save_plot('02_train_eval_split.png')\n",
    "\n",
    "# 3. Recordings per Patient\n",
    "if len(recordings_per_patient) > 1:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(recordings_per_patient, bins=min(30, max(recordings_per_patient)), color='#9b59b6', alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Recordings per Patient', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Number of Patients', fontsize=12, fontweight='bold')\n",
    "    plt.title(f'Patient Recording Distribution\\n(Î¼={np.mean(recordings_per_patient):.1f}, Ïƒ={np.std(recordings_per_patient):.1f})',\n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.axvline(np.mean(recordings_per_patient), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    save_plot('03_recordings_per_patient.png')\n",
    "\n",
    "# 4. Seizure Type Distribution\n",
    "if seizure_type_counts:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    types = list(seizure_type_counts.keys())[:10]\n",
    "    counts = [seizure_type_counts[t] for t in types]\n",
    "    plt.barh(types, counts, color='#e67e22', alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Count', fontsize=12, fontweight='bold')\n",
    "    plt.title('Seizure Types (Top 10)', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    save_plot('04_seizure_types.png')\n",
    "\n",
    "# 5. Seizure Duration Distribution\n",
    "if seizure_data['seizure_durations']:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(seizure_data['seizure_durations'], bins=40, color='#c0392b', alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Duration (seconds)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    mean_dur = np.mean(seizure_data['seizure_durations'])\n",
    "    median_dur = np.median(seizure_data['seizure_durations'])\n",
    "    plt.title(f'Seizure Duration Distribution\\n(Î¼={mean_dur:.1f}s, median={median_dur:.1f}s)',\n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.axvline(mean_dur, color='yellow', linestyle='--', linewidth=2, label='Mean')\n",
    "    plt.axvline(median_dur, color='green', linestyle='--', linewidth=2, label='Median')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    save_plot('05_seizure_duration.png')\n",
    "\n",
    "# 6. Recording Duration Distribution\n",
    "if seizure_data['recording_durations']:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(seizure_data['recording_durations'], bins=40, color='#16a085', alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Duration (seconds)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    mean_rec = np.mean(seizure_data['recording_durations'])\n",
    "    plt.title(f'Recording Duration Distribution\\n(Î¼={mean_rec:.1f}s)',\n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.axvline(mean_rec, color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    save_plot('06_recording_duration.png')\n",
    "\n",
    "# 7. Most Active Channels (Seizure Activity)\n",
    "if seizure_data['channels_involved']:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_channels = sorted(seizure_data['channels_involved'].items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "    channels = [ch[0] for ch in top_channels]\n",
    "    counts = [ch[1] for ch in top_channels]\n",
    "    plt.barh(channels, counts, color='#8e44ad', alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Seizure Event Count', fontsize=12, fontweight='bold')\n",
    "    plt.title('Top 15 Channels with Seizure Activity', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    save_plot('07_active_channels.png')\n",
    "\n",
    "# 8. Sampling Rate Distribution\n",
    "if edf_metadata['sampling_rates']:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sampling_rate_counts = Counter(edf_metadata['sampling_rates'])\n",
    "    rates = sorted(list(sampling_rate_counts.keys()))\n",
    "    counts = [sampling_rate_counts[r] for r in rates]\n",
    "    plt.bar(rates, counts, color='#27ae60', alpha=0.7, edgecolor='black', width=20)\n",
    "    plt.xlabel('Sampling Rate (Hz)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Number of Recordings', fontsize=12, fontweight='bold')\n",
    "    plt.title(f'Sampling Rate Distribution\\n(Most common: {max(sampling_rate_counts, key=sampling_rate_counts.get)} Hz)',\n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    save_plot('08_sampling_rates.png')\n",
    "\n",
    "# 9. Number of Channels per Recording\n",
    "if edf_metadata['num_channels']:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(edf_metadata['num_channels'], bins=20, color='#2980b9', alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Number of Channels', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Number of Recordings', fontsize=12, fontweight='bold')\n",
    "    mean_ch = np.mean(edf_metadata['num_channels'])\n",
    "    plt.title(f'Channel Count Distribution\\n(Î¼={mean_ch:.1f} channels)',\n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.axvline(mean_ch, color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    save_plot('09_channel_count.png')\n",
    "\n",
    "# 10. Patient Age Distribution\n",
    "if edf_metadata['patient_ages']:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(edf_metadata['patient_ages'], bins=30, color='#d35400', alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Age (years)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Number of Patients', fontsize=12, fontweight='bold')\n",
    "    mean_age = np.mean(edf_metadata['patient_ages'])\n",
    "    plt.title(f'Patient Age Distribution\\n(Î¼={mean_age:.1f} years, range={min(edf_metadata[\"patient_ages\"])}-{max(edf_metadata[\"patient_ages\"])})',\n",
    "               fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.axvline(mean_age, color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    save_plot('10_patient_age.png')\n",
    "\n",
    "# 11. Patient Gender Distribution\n",
    "if edf_metadata['patient_genders']:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    gender_counts = Counter(edf_metadata['patient_genders'])\n",
    "    genders = list(gender_counts.keys())\n",
    "    counts = list(gender_counts.values())\n",
    "    colors_gender = ['#3498db' if g == 'M' else '#e91e63' for g in genders]\n",
    "    bars = plt.bar(genders, counts, color=colors_gender, alpha=0.7, edgecolor='black')\n",
    "    plt.ylabel('Count', fontsize=12, fontweight='bold')\n",
    "    plt.title('Patient Gender Distribution', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{int(height)}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    save_plot('11_patient_gender.png')\n",
    "\n",
    "# 12. Sessions per Patient\n",
    "if len(sessions_per_patient) > 1:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(sessions_per_patient, bins=min(20, max(sessions_per_patient)), color='#1abc9c', alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Sessions per Patient', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Number of Patients', fontsize=12, fontweight='bold')\n",
    "    mean_sess = np.mean(sessions_per_patient)\n",
    "    plt.title(f'Patient Session Distribution\\n(Î¼={mean_sess:.1f} sessions)',\n",
    "               fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.axvline(mean_sess, color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    save_plot('12_sessions_per_patient.png')\n",
    "\n",
    "# 13. File Size Distribution by Type\n",
    "file_size_data = []\n",
    "file_size_labels = []\n",
    "for ftype, sizes in dataset_stats['file_sizes'].items():\n",
    "    if sizes:\n",
    "        file_size_data.extend(sizes)\n",
    "        file_size_labels.extend([ftype.upper()] * len(sizes))\n",
    "\n",
    "if file_size_data:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    size_df = pd.DataFrame({'Size (MB)': file_size_data, 'Type': file_size_labels})\n",
    "    sns.boxplot(data=size_df, x='Type', y='Size (MB)', palette='Set2')\n",
    "    plt.title('File Size Distribution by Type', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.ylabel('Size (MB)', fontsize=12, fontweight='bold')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    save_plot('13_file_sizes.png')\n",
    "\n",
    "# 14. Seizure vs Background Time Ratio\n",
    "if seizure_data['seizure_durations'] and seizure_data['background_durations']:\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    total_seizure = sum(seizure_data['seizure_durations'])\n",
    "    total_background = sum(seizure_data['background_durations'][:len(seizure_data['seizure_durations'])])  # Match sample size\n",
    "    data = [total_seizure, total_background]\n",
    "    labels = [f'Seizure\\n({total_seizure:.0f}s)', f'Background\\n({total_background:.0f}s)']\n",
    "    colors_pie = ['#e74c3c', '#95a5a6']\n",
    "    wedges, texts, autotexts = plt.pie(data, labels=labels, autopct='%1.1f%%',\n",
    "                                         colors=colors_pie, startangle=90)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "        autotext.set_fontsize(11)\n",
    "    plt.title('Seizure vs Background Time\\n(Class Balance)', fontsize=14, fontweight='bold', pad=20)\n",
    "    save_plot('14_class_balance.png')\n",
    "\n",
    "# 15. Dataset Quality Metrics Table\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "quality_metrics = [\n",
    "    ['Total Patients', f\"{len(dataset_stats['patients']):,}\"],\n",
    "    ['Total Recordings', f\"{len(dataset_stats['edf_files']):,}\"],\n",
    "    ['Avg Recordings/Patient', f\"{np.mean(recordings_per_patient):.1f}\"],\n",
    "    ['Seizure Events Detected', f\"{len(seizure_data['seizure_events']):,}\"],\n",
    "    ['Unique Seizure Types', f\"{len(seizure_type_counts)}\"],\n",
    "    ['Avg Seizure Duration', f\"{np.mean(seizure_data['seizure_durations']):.1f}s\" if seizure_data['seizure_durations'] else \"N/A\"],\n",
    "    ['Avg Recording Duration', f\"{np.mean(seizure_data['recording_durations']):.1f}s\" if seizure_data['recording_durations'] else \"N/A\"],\n",
    "    ['Most Common Samp. Rate', f\"{max(Counter(edf_metadata['sampling_rates']), key=Counter(edf_metadata['sampling_rates']).get)} Hz\" if edf_metadata['sampling_rates'] else \"N/A\"],\n",
    "]\n",
    "\n",
    "table = ax.table(cellText=quality_metrics, colWidths=[0.6, 0.4],\n",
    "                  cellLoc='left', loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2.5)\n",
    "for i in range(len(quality_metrics)):\n",
    "    cell = table[(i, 0)]\n",
    "    cell.set_facecolor('#ecf0f1' if i % 2 == 0 else 'white')\n",
    "    cell.set_text_props(weight='bold')\n",
    "    cell = table[(i, 1)]\n",
    "    cell.set_facecolor('#ecf0f1' if i % 2 == 0 else 'white')\n",
    "ax.set_title('Key Dataset Metrics', fontsize=14, fontweight='bold', pad=20)\n",
    "save_plot('15_summary_metrics.png')\n",
    "\n",
    "print(f\"\\nâœ“ All visualizations saved to '{OUTPUT_DIR}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook-venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
